#import crawler.properties.defaults
##### THESE ARE CRAWLER SETTINGS

#crawler mode, can be: redistribute, cycles, runforever, refresh
#crawler.mode=runforever
#crawler.mode.skip=0
#only valid if crawler.mode=cycles
#crawler.mode.cycles=2

# The name of the crawl, to be added to the index to restrict the search to the results of this crawl.
crawler.name = test

# The number of web pages to fetch per fetch cycle.
fetchlist.size = 500

# The number of pages to fetch outside of the hotspot vicinity.
# These pages will only be kept if they become hotspots, there is no distance limit for them, and retries is 0.
discovery.front.size = 0

# The percentile of pages from the pagedb that should be fetched on a crawl cycle based on their priority.
priority.percentile.to.fetch = 10

# Modules to use after a page has been fetched. The crawler will run every module, in the order specified here. The list has to be a pipe-separated list of modules, where each module is a comma-separated pair of class and name. The configuration  of each module will be read from files with the name of the module, concatenated  to "Module.properties". This allows to have more than one instance of the same class with different configuration for each instance.

modules = com.flaptor.hounder.crawler.modules.PatternClassifierModule,pattern|\
          com.flaptor.hounder.crawler.modules.IndexerModule,indexer|\
          com.flaptor.hounder.crawler.modules.CacheModule,cache|\
          com.flaptor.hounder.crawler.modules.LoggerModule,logger

hotspot.tag = hotspot
emitdoc.tag = emitdoc

pagedb.is.distributed = false
pagedb.node.list =
pagedb.node.mapper = com.flaptor.hounder.crawler.pagedb.distributed.UrlHashMapper
injected.pagedb.dir = injectdb

#these lists control the connections accepted from clusterfest
#you can add many values with commas and use wildcards as in 192.168.*.* 
clustering.node.connections.accept=
clustering.node.connections.deny=
